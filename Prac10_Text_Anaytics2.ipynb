{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7995fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('titanic.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c65f482",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text=df[\"Name\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4df72ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Braund, Mr. Owen Harris'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03006f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amrut\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "print(stop_words)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import FreqDist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce34b1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a boy and a girl were playing together. the boy had a collection of marbles. the girl has some sweets with her. the boy told the girl that he would give her all his marbles in exchange for the sweets with her. the girl agreed.the boy kept the most beautiful and the biggest marbles with him and gave her the remaining marbles. the girl gave him all her sweets as she promised. that night the girl slept peacefully. but the boy could not sleep as he kept wondering if the girl has hidden some sweets from him the way he had hidden the best marbles from her.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"A boy and a girl were playing together. The boy had a collection of marbles. The girl has some sweets with her. The boy told the girl that he would give her all his marbles in exchange for the sweets with her. The girl agreed.The boy kept the most beautiful and the biggest marbles with him and gave her the remaining marbles. The girl gave him all her sweets as she promised. That night the girl slept peacefully. But the boy could not sleep as he kept wondering if the girl has hidden some sweets from him the way he had hidden the best marbles from her.\"\n",
    "text = text.lower()\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cfdad2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amrut\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is word wise tokenization-: \n",
      " ['CSI-DYPIEMR', 'is', 'the', 'Student', 'Chapter', 'of', 'Computer', 'Society', 'of', 'India', 'in', 'Dr.D', '.', 'Y.', 'Patil', 'Pratishthan', \"'s\", 'Dr.', 'D.', 'Y.', 'Patil', 'Institute', 'of', 'Engineering', ',', 'Management', ',', 'and', 'Research', '.', 'Computer', 'Society', 'of', 'India', 'is', 'a', 'body', 'of', 'computer', 'professionals', 'in', 'India', '.', 'It', 'was', 'started', 'on', '6', 'March', '1965', 'by', 'a', 'few', 'computer', 'professionals', 'and', 'has', 'now', 'grown', 'to', 'be', 'the', 'national', 'body', 'representing', 'computer', 'professionals', '.', 'It', 'has', '72', 'chapters', 'across', 'India', ',', '511', 'student', 'branches', ',', 'and', '100,000', 'members', '.'] \n",
      "\n",
      "-----------------------------------------------------------------------------'\n",
      "'\n",
      "This is sentence wise tokenization-: \n",
      " ['CSI-DYPIEMR is the Student Chapter of Computer Society of India in Dr.D.', \"Y. Patil Pratishthan's Dr. D. Y. Patil Institute of Engineering,Management, and Research.\", 'Computer Society of India is a body of computer professionals in India.', 'It was started on 6 March 1965 by a few computer professionals and has now grown to be the national body representing computer professionals.', 'It has 72 chapters across India, 511 student branches, and 100,000 members.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "block = \"CSI-DYPIEMR is the Student Chapter of Computer Society of India in Dr.D. Y. Patil Pratishthan's Dr. D. Y. Patil Institute of Engineering,Management, and Research. Computer Society of India is a body of computer professionals in India. It was started on 6 March 1965 by a few computer professionals and has now grown to be the national body representing computer professionals. It has 72 chapters across India, 511 student branches, and 100,000 members.\"\n",
    "print(\"This is word wise tokenization-:\",'\\n', nltk.word_tokenize(block), '\\n')\n",
    "print(\"-----------------------------------------------------------------------------'\\n'\")\n",
    "print(\"This is sentence wise tokenization-:\",'\\n', nltk.sent_tokenize(block))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "028c44a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "This is the unclean version-: \n",
      " ['CSI-DYPIEMR', 'is', 'the', 'Student', 'Chapter', 'of', 'Computer', 'Society', 'of', 'India', 'in', 'Dr.D', '.', 'Y.', 'Patil', 'Pratishthan', \"'s\", 'Dr.', 'D.', 'Y.', 'Patil', 'Institute', 'of', 'Engineering', ',', 'Management', ',', 'and', 'Research', '.', 'Computer', 'Society', 'of', 'India', 'is', 'a', 'body', 'of', 'computer', 'professionals', 'in', 'India', '.', 'It', 'was', 'started', 'on', '6', 'March', '1965', 'by', 'a', 'few', 'computer', 'professionals', 'and', 'has', 'now', 'grown', 'to', 'be', 'the', 'national', 'body', 'representing', 'computer', 'professionals', '.', 'It', 'has', '72', 'chapters', 'across', 'India', ',', '511', 'student', 'branches', ',', 'and', '100,000', 'members', '.'] \n",
      "\n",
      "----------------------------------------------------------------------'\n",
      "'\n",
      "This is the cleaned version-: \n",
      " ['CSI-DYPIEMR', 'Student', 'Chapter', 'Computer', 'Society', 'India', 'Dr.D', '.', 'Y.', 'Patil', 'Pratishthan', \"'s\", 'Dr.', 'D.', 'Y.', 'Patil', 'Institute', 'Engineering', ',', 'Management', ',', 'Research', '.', 'Computer', 'Society', 'India', 'body', 'computer', 'professionals', 'India', '.', 'It', 'started', '6', 'March', '1965', 'computer', 'professionals', 'grown', 'national', 'body', 'representing', 'computer', 'professionals', '.', 'It', '72', 'chapters', 'across', 'India', ',', '511', 'student', 'branches', ',', '100,000', 'members', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amrut\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)\n",
    "token = nltk.word_tokenize(block)\n",
    "cleaned_token = []\n",
    "for word in token:\n",
    "    if word not in stop_words:\n",
    "        cleaned_token.append(word)\n",
    "print(\"This is the unclean version-:\",'\\n', token, '\\n')\n",
    "print(\"----------------------------------------------------------------------'\\n'\")\n",
    "print(\"This is the cleaned version-:\",'\\n', cleaned_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7ed5416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rain', 'rain', 'rain', 'rain']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = nltk.PorterStemmer()\n",
    "words = ['rain', 'rained', 'raining', 'rains']\n",
    "stemmed = [stemmer.stem(word) for word in words]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1113709d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\amrut\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\amrut\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CSI-DYPIEMR', 'Student', 'Chapter', 'Computer', 'Society', 'India', 'Dr.D', '.', 'Y.', 'Patil', 'Pratishthan', \"'s\", 'Dr.', 'D.', 'Y.', 'Patil', 'Institute', 'Engineering', ',', 'Management', ',', 'Research', '.', 'Computer', 'Society', 'India', 'body', 'computer', 'professional', 'India', '.', 'It', 'started', '6', 'March', '1965', 'computer', 'professional', 'grown', 'national', 'body', 'representing', 'computer', 'professional', '.', 'It', '72', 'chapter', 'across', 'India', ',', '511', 'student', 'branch', ',', '100,000', 'member', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')#data dependencies\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in cleaned_token]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1f5a1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\amrut\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CSI-DYPIEMR', 'JJ'), ('Student', 'NNP'), ('Chapter', 'NNP'), ('Computer', 'NNP'), ('Society', 'NNP'), ('India', 'NNP'), ('Dr.D', 'NNP'), ('.', '.'), ('Y.', 'NNP'), ('Patil', 'NNP'), ('Pratishthan', 'NNP'), (\"'s\", 'POS'), ('Dr.', 'NNP'), ('D.', 'NNP'), ('Y.', 'NNP'), ('Patil', 'NNP'), ('Institute', 'NNP'), ('Engineering', 'NNP'), (',', ','), ('Management', 'NNP'), (',', ','), ('Research', 'NNP'), ('.', '.'), ('Computer', 'NNP'), ('Society', 'NNP'), ('India', 'NNP'), ('body', 'NN'), ('computer', 'NN'), ('professionals', 'NNS'), ('India', 'NNP'), ('.', '.'), ('It', 'PRP'), ('started', 'VBD'), ('6', 'CD'), ('March', 'NNP'), ('1965', 'CD'), ('computer', 'NN'), ('professionals', 'NNS'), ('grown', 'VBP'), ('national', 'JJ'), ('body', 'NN'), ('representing', 'VBG'), ('computer', 'NN'), ('professionals', 'NNS'), ('.', '.'), ('It', 'PRP'), ('72', 'CD'), ('chapters', 'NNS'), ('across', 'IN'), ('India', 'NNP'), (',', ','), ('511', 'CD'), ('student', 'NN'), ('branches', 'NNS'), (',', ','), ('100,000', 'CD'), ('members', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "tagged = nltk.pos_tag(cleaned_token)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3524baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1782133d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'good', 'conventions,', 'same', 'of', 'students', 'priority', 'with', 'profession', 'Our', 'among', 'aim', 'as', 'backgrounds', 'area', 'regular', 'lectures,', 'the', 'collaborate', 'To', 'from', 'awards.', 'fulfill', 'work', 'promotion', 'are', 'conferences,', 'computer', 'training', 'organized', 'where', 'that', 'develop', 'IT', 'is', 'a', 'The', 'today.', 'all', 'making', 'enthusiasts,', 'regularly', 'students,', 'teach,guide', 'And', 'objective,the', 'projects', 'ensures', 'future', 'works', 'time,', 'Keeping', 'interest', 'CSI', 'towards', 'updating', 'amongst', 'grow', 'other', 'an', 'skill', 'sections', 'to', 'professionals.', 'technical', 'at', 'mind', 'in', 'each', 'Technology', 'for', 'culture', 'professionals', 'projects,and', 'various', 'choice', 'society.', 'come', 'it', 'together.', 'this', 'together', 'organizes', 'Information', 'and', 'on', 'also', 'top'}\n"
     ]
    }
   ],
   "source": [
    "block_1 = \"Our aim is to develop a good work culture among students, a culture where students from various technical backgrounds come together to teach,guide and collaborate with each other on various projects and grow together.\"\n",
    "block_2 = \"Keeping in mind the interest of the IT professionals and computer enthusiasts, CSI works towards making the profession an area of choice amongst all sections of the society. The promotion of Information Technology as a profession is the top priority of CSI today. To fulfill this objective,the CSI regularly organizes conferences, conventions, lectures, projects,and awards. And at the same time, it also ensures that regular training and skill updating are organized for the future IT professionals.\"\n",
    "#split so each word have their own string\n",
    "first_block = block_1.split(\" \")\n",
    "second_block = block_2.split(\" \")\n",
    "#join them to remove common duplicate words\n",
    "total= set(first_block).union(set(second_block))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dbf0543",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDictA = dict.fromkeys(total, 0)\n",
    "wordDictB = dict.fromkeys(total, 0)\n",
    "for word in first_block:\n",
    "    wordDictA[word]+=1\n",
    "for word in second_block:\n",
    "    wordDictB[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccb96649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>good</th>\n",
       "      <th>conventions,</th>\n",
       "      <th>same</th>\n",
       "      <th>of</th>\n",
       "      <th>students</th>\n",
       "      <th>priority</th>\n",
       "      <th>with</th>\n",
       "      <th>profession</th>\n",
       "      <th>Our</th>\n",
       "      <th>among</th>\n",
       "      <th>...</th>\n",
       "      <th>it</th>\n",
       "      <th>together.</th>\n",
       "      <th>this</th>\n",
       "      <th>together</th>\n",
       "      <th>organizes</th>\n",
       "      <th>Information</th>\n",
       "      <th>and</th>\n",
       "      <th>on</th>\n",
       "      <th>also</th>\n",
       "      <th>top</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   good  conventions,  same  of  students  priority  with  profession  Our  \\\n",
       "0     1             0     0   0         1         0     1           0    1   \n",
       "1     0             1     1   5         0         1     0           2    0   \n",
       "\n",
       "   among  ...  it  together.  this  together  organizes  Information  and  on  \\\n",
       "0      1  ...   0          1     0         1          0            0    2   1   \n",
       "1      0  ...   1          0     1         0          1            1    2   0   \n",
       "\n",
       "   also  top  \n",
       "0     0    0  \n",
       "1     1    1  \n",
       "\n",
       "[2 rows x 87 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40a1a814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'conventions,', 'students', 'priority', 'profession', 'Our', 'among', 'aim', 'backgrounds', 'area', 'regular', 'lectures,', 'collaborate', 'To', 'awards.', 'fulfill', 'work', 'promotion', 'conferences,', 'computer', 'training', 'organized', 'develop', 'IT', 'The', 'today.', 'making', 'enthusiasts,', 'regularly', 'students,', 'teach,guide', 'And', 'objective,the', 'projects', 'ensures', 'future', 'works', 'time,', 'Keeping', 'interest', 'CSI', 'towards', 'updating', 'amongst', 'grow', 'skill', 'sections', 'professionals.', 'technical', 'mind', 'Technology', 'culture', 'professionals', 'projects,and', 'various', 'choice', 'society.', 'come', 'together.', 'together', 'organizes', 'Information', 'also', 'top']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amrut\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_sentence = [w for w in wordDictA if not w in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7632201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       good  conventions,      same        of  students  priority      with  \\\n",
      "0  0.029412      0.000000  0.000000  0.000000  0.029412  0.000000  0.029412   \n",
      "1  0.000000      0.013158  0.013158  0.065789  0.000000  0.013158  0.000000   \n",
      "\n",
      "   profession       Our     among  ...        it  together.      this  \\\n",
      "0    0.000000  0.029412  0.029412  ...  0.000000   0.029412  0.000000   \n",
      "1    0.026316  0.000000  0.000000  ...  0.013158   0.000000  0.013158   \n",
      "\n",
      "   together  organizes  Information       and        on      also       top  \n",
      "0  0.029412   0.000000     0.000000  0.058824  0.029412  0.000000  0.000000  \n",
      "1  0.000000   0.013158     0.013158  0.026316  0.000000  0.013158  0.013158  \n",
      "\n",
      "[2 rows x 87 columns]\n"
     ]
    }
   ],
   "source": [
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)\n",
    "#running our sentences through the tf function:\n",
    "tfFirst = computeTF(wordDictA, first_block)\n",
    "tfSecond = computeTF(wordDictB, second_block)\n",
    "tf = pd.DataFrame([tfFirst, tfSecond])\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea901ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   good  conventions,  same  of  students  priority  with  profession  Our  \\\n",
      "0     1             0     0   0         1         0     1           0    1   \n",
      "1     0             1     1   5         0         1     0           2    0   \n",
      "\n",
      "   among  ...  it  together.  this  together  organizes  Information  and  on  \\\n",
      "0      1  ...   0          1     0         1          0            0    2   1   \n",
      "1      0  ...   1          0     1         0          1            1    2   0   \n",
      "\n",
      "   also  top  \n",
      "0     0    0  \n",
      "1     1    1  \n",
      "\n",
      "[2 rows x 87 columns]\n"
     ]
    }
   ],
   "source": [
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for word, val in idfDict.items(): idfDict[word] = math.log10(N /(float(val) + 1))\n",
    "    return(idfDict)\n",
    "\n",
    "idfs = computeIDF([wordDictA, wordDictB])\n",
    "idfs1 = pd.DataFrame([wordDictA, wordDictB])\n",
    "print(idfs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3a38b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       good  conventions,      same        of  students  priority      with  \\\n",
      "0  0.008854      0.000000  0.000000  0.000000  0.008854  0.000000  0.008854   \n",
      "1  0.000000      0.003961  0.003961  0.019805  0.000000  0.003961  0.000000   \n",
      "\n",
      "   profession       Our     among  ...        it  together.      this  \\\n",
      "0    0.000000  0.008854  0.008854  ...  0.000000   0.008854  0.000000   \n",
      "1    0.007922  0.000000  0.000000  ...  0.003961   0.000000  0.003961   \n",
      "\n",
      "   together  organizes  Information       and        on      also       top  \n",
      "0  0.008854   0.000000     0.000000  0.017708  0.008854  0.000000  0.000000  \n",
      "1  0.000000   0.003961     0.003961  0.007922  0.000000  0.003961  0.003961  \n",
      "\n",
      "[2 rows x 87 columns]\n"
     ]
    }
   ],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items(): tfidf[word] = val*idfs[word]\n",
    "    return(tfidf)\n",
    "#running our two sentences through the IDF:\n",
    "idfFirst = computeTFIDF(tfFirst, idfs)\n",
    "idfSecond = computeTFIDF(tfSecond, idfs)\n",
    "#putting it in a dataframe\n",
    "idf= pd.DataFrame([idfFirst, idfSecond])\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d408cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#Make sure all words are in lowercase\n",
    "version_1 = \"Developing a competitive culture where the students polish technical and professional attributes, gain experience and learn new skills while upgrading the already present skillset. For those fledglings who have a zeal to build a strong profile and are hunting for their Ikigai, CSI provides ample opportunities for those individuals too.\"\n",
    "version_2 = \"Personalized career guidance, Regular Logic and aptitude building activities, Industrial level project collaboration, Building a network with active collaborations across the globe, Periodic member exclusive conferences and seminars, Created a community for sharing skills and knowledge\"\n",
    "#calling the TfidfVectorizer\n",
    "vectorize= TfidfVectorizer()\n",
    "#fitting the model and passing our sentences right away:\n",
    "response= vectorize.fit_transform([version_1.lower(), version_2.lower()])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26a0c55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 61)\t0.13915271943780658\n",
      "  (0, 31)\t0.13915271943780658\n",
      "  (0, 40)\t0.13915271943780658\n",
      "  (0, 4)\t0.13915271943780658\n",
      "  (0, 48)\t0.13915271943780658\n",
      "  (0, 18)\t0.13915271943780658\n",
      "  (0, 30)\t0.13915271943780658\n",
      "  (0, 58)\t0.13915271943780658\n",
      "  (0, 29)\t0.13915271943780658\n",
      "  (0, 7)\t0.13915271943780658\n",
      "  (0, 46)\t0.13915271943780658\n",
      "  (0, 54)\t0.13915271943780658\n",
      "  (0, 9)\t0.13915271943780658\n",
      "  (0, 60)\t0.13915271943780658\n",
      "  (0, 67)\t0.13915271943780658\n",
      "  (0, 28)\t0.13915271943780658\n",
      "  (0, 65)\t0.13915271943780658\n",
      "  (0, 23)\t0.13915271943780658\n",
      "  (0, 59)\t0.27830543887561315\n",
      "  (0, 24)\t0.2970249178760062\n",
      "  (0, 53)\t0.13915271943780658\n",
      "  (0, 44)\t0.13915271943780658\n",
      "  (0, 3)\t0.13915271943780658\n",
      "  (0, 62)\t0.13915271943780658\n",
      "  (0, 64)\t0.13915271943780658\n",
      "  :\t:\n",
      "  (1, 21)\t0.16649349332910351\n",
      "  (1, 37)\t0.16649349332910351\n",
      "  (1, 41)\t0.16649349332910351\n",
      "  (1, 26)\t0.16649349332910351\n",
      "  (1, 0)\t0.16649349332910351\n",
      "  (1, 13)\t0.16649349332910351\n",
      "  (1, 1)\t0.16649349332910351\n",
      "  (1, 66)\t0.16649349332910351\n",
      "  (1, 38)\t0.16649349332910351\n",
      "  (1, 12)\t0.16649349332910351\n",
      "  (1, 47)\t0.16649349332910351\n",
      "  (1, 35)\t0.16649349332910351\n",
      "  (1, 32)\t0.16649349332910351\n",
      "  (1, 2)\t0.16649349332910351\n",
      "  (1, 10)\t0.33298698665820703\n",
      "  (1, 6)\t0.16649349332910351\n",
      "  (1, 36)\t0.16649349332910351\n",
      "  (1, 49)\t0.16649349332910351\n",
      "  (1, 27)\t0.16649349332910351\n",
      "  (1, 11)\t0.16649349332910351\n",
      "  (1, 42)\t0.16649349332910351\n",
      "  (1, 24)\t0.11846149176425531\n",
      "  (1, 52)\t0.11846149176425531\n",
      "  (1, 5)\t0.35538447529276596\n",
      "  (1, 57)\t0.11846149176425531\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddf8730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
